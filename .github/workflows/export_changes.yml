name: Export Changes

on:
  workflow_call:
    inputs:
      bucket_name:
        required: true
        type: string
      aws_region:
        required: true
        type: string
      include_code:
        required: false
        type: string
        default: ''
      include_documentation:
        required: false
        type: string
        default: ''
      exclude_code:
        required: false
        type: string
        default: ''
      exclude_documentation:
        required: false
        type: string
        default: ''
    secrets:
      AWS_ACCESS_KEY_ID:
        required: true
      AWS_SECRET_ACCESS_KEY:
        required: true

jobs:
  export-structure:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set GitHub Variables
        run: |
          COMMIT_ID="${GITHUB_SHA}"
          
          # Ensure we have a commit SHA
          if [[ -z "$COMMIT_ID" ]]; then
            echo "No commit SHA found. Using latest commit from main branch."
            COMMIT_ID=$(git rev-parse HEAD)
          fi
          
          # Get commit date in YYYY/MM/DD format
          COMMIT_DATE=$(git show -s --format=%cd --date=format:'%Y/%m/%d' "$COMMIT_ID")
          echo "COMMIT_ID=$COMMIT_ID" >> $GITHUB_ENV
          echo "COMMIT_DATE=$COMMIT_DATE" >> $GITHUB_ENV
          
          BASE_S3_PATH="s3://${{ inputs.bucket_name }}/${{ github.event.repository.name }}"
          echo "BASE_S3_PATH=$BASE_S3_PATH" >> $GITHUB_ENV

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ inputs.aws_region }}

#      - name: Check if S3 code and documentation folders are empty
#        id: check_empty
#        run: |
#          CODE_COUNT=$(aws s3 ls "$BASE_S3_PATH/code/" | wc -l)
#          DOC_COUNT=$(aws s3 ls "$BASE_S3_PATH/documentation/" | wc -l)
#          if [ "$CODE_COUNT" -eq 0 ] && [ "$DOC_COUNT" -eq 0 ]; then
#            echo "empty=true" >> $GITHUB_OUTPUT
#            echo "No files found in S3 code and documentation folders."
#          else
#            echo "empty=false" >> $GITHUB_OUTPUT
#            echo "Files found in S3 code and documentation folders."
#          fi

      - name: Sync of code and documentation to S3
        # if: steps.check_empty.outputs.empty == 'true'
        run: |
          set -e

          to_patterns() {
            local input="$1" # Input string, can be a comma-separated list
            local flag="$2" # Flag to use for include/exclude
            local patterns=""
            for item in $(echo "$input" | tr ',' ' '); do
              item=$(echo "$item" | xargs)
              if [[ -z "$item" ]]; then continue; fi
              if [[ "$item" == */ ]]; then
                # Handle directories with trailing slash
                patterns="$patterns $flag\"${item%/}\" $flag\"${item%/}/*\""
              elif [[ "$item" == .* ]]; then
                # Handle hidden files or directories
                patterns="$patterns $flag\"*${item}\""
              elif [[ "$item" == *.* ]]; then
                # Handle files with extensions
                patterns="$patterns $flag\"${item}\""
              else
                # Handle directories without trailing slash
                patterns="$patterns $flag\"${item}/*\" $flag\"${item}\""
              fi
            done
            echo "$patterns"
          }

          CODE_INCLUDE="${{ inputs.include_code }}"
          CODE_EXCLUDE="${{ inputs.exclude_code }}"
          DOC_INCLUDE="${{ inputs.include_documentation }}"
          DOC_EXCLUDE="${{ inputs.exclude_documentation }}"

          # Code sync
          if [[ -z "$CODE_INCLUDE" ]]; then
            CODE_EXCLUDE_PATTERNS=$(to_patterns "$CODE_EXCLUDE" --exclude\ )
#            echo "ðŸ§¹ Cleaning S3 code destination..."
#            aws s3 rm "$BASE_S3_PATH/code/" --recursive || true
            echo "ðŸš€ Syncing all code files except excluded..."
            eval aws s3 sync . "$BASE_S3_PATH/code/" $CODE_EXCLUDE_PATTERNS --delete
          else
            CODE_INCLUDE_PATTERNS=$(to_patterns "$CODE_INCLUDE" --include\ )
            CODE_EXCLUDE_PATTERNS=$(to_patterns "$CODE_EXCLUDE" --exclude\ )
#            echo "ðŸ§¹ Cleaning S3 code destination..."
#            aws s3 rm "$BASE_S3_PATH/code/" --recursive || true
            echo "ðŸš€ Syncing filtered code files..."
            eval aws s3 sync . "$BASE_S3_PATH/code/" --exclude "*" $CODE_INCLUDE_PATTERNS $CODE_EXCLUDE_PATTERNS --delete
          fi

          # Documentation sync
          if [[ -z "$DOC_INCLUDE" ]]; then
            DOC_EXCLUDE_PATTERNS=$(to_patterns "$DOC_EXCLUDE" --exclude\ )
#            echo "ðŸ§¹ Cleaning S3 documentation destination..."
#            aws s3 rm "$BASE_S3_PATH/documentation/" --recursive || true
            echo "ðŸš€ Syncing all documentation files except excluded..."
            eval aws s3 sync . "$BASE_S3_PATH/documentation/" $DOC_EXCLUDE_PATTERNS --delete
          else
            DOC_INCLUDE_PATTERNS=$(to_patterns "$DOC_INCLUDE" --include\ )
            DOC_EXCLUDE_PATTERNS=$(to_patterns "$DOC_EXCLUDE" --exclude\ )
#            echo "ðŸ§¹ Cleaning S3 documentation destination..."
#            aws s3 rm "$BASE_S3_PATH/documentation/" --recursive || true
            echo "ðŸš€ Syncing filtered documentation files..."
            eval aws s3 sync . "$BASE_S3_PATH/documentation/" --exclude "*" $DOC_INCLUDE_PATTERNS $DOC_EXCLUDE_PATTERNS --delete
          fi

#      - name: Upload to S3 only code and documentation changed in last commit
#        if: steps.check_empty.outputs.empty == 'false'
#        run: |
#          set -e
#
#          CODE_INCLUDE="${{ inputs.include_code }}"
#          CODE_EXCLUDE="${{ inputs.exclude_code }}"
#          DOC_INCLUDE="${{ inputs.include_documentation }}"
#          DOC_EXCLUDE="${{ inputs.exclude_documentation }}"
#
#          should_include() {
#            local path="$1"
#            local includes="$2"
#            local excludes="$3"
#
#            includes=$(echo "$includes" | tr ',' ' ')
#            excludes=$(echo "$excludes" | tr ',' ' ')
#
#            for item in $excludes; do
#              item=$(echo "$item" | xargs)
#              if [[ "$item" == .* && "$path" == *"$item" ]]; then
#                return 1
#              elif [[ "$item" == */ && "$path" == "$item"* ]]; then
#                return 1
#              elif [[ "$path" == "$item" ]] || [[ "$path" == "$item/"* ]]; then
#                return 1
#              fi
#            done
#
#            if [[ -z "$includes" ]]; then
#              return 0
#            fi
#
#            for item in $includes; do
#              item=$(echo "$item" | xargs)
#              if [[ "$item" == .* && "$path" == *"$item" ]]; then
#                return 0
#              elif [[ "$item" == */ && "$path" == "$item"* ]]; then
#                return 0
#              elif [[ "$path" == "$item" ]] || [[ "$path" == "$item/"* ]]; then
#                return 0
#              fi
#            done
#
#            return 1
#          }
#
#          echo "ðŸ” Checking changed files in last commit..."
#          git diff --name-only $COMMIT_ID^ $COMMIT_ID | while read file; do
#            clean_path="${file#./}"
#            if [[ -f "$file" ]]; then
#              if should_include "$clean_path" "$CODE_INCLUDE" "$CODE_EXCLUDE"; then
#                echo "âœ… Upload CODE: $clean_path"
#                aws s3 cp "$file" "$BASE_S3_PATH/code/$clean_path"
#              elif should_include "$clean_path" "$DOC_INCLUDE" "$DOC_EXCLUDE"; then
#                echo "âœ… Upload DOCUMENTATION: $clean_path"
#                aws s3 cp "$file" "$BASE_S3_PATH/documentation/$clean_path"
#              else
#                echo "â­ï¸ Skipped: $clean_path"
#              fi
#            fi
#          done

      - name: Generate repository structure
        run: |
          git ls-tree -r --name-only HEAD > repo_structure_${{ env.COMMIT_ID }}.txt  

      - name: Upload repository structure as artifact
        uses: actions/upload-artifact@v4
        with:
          name: repository-structure
          path: repo_structure_${{ env.COMMIT_ID }}.txt

      - name: Generate commit history
        run: |
          echo "Hash,Author,Date,Message" > commit-history.csv
          git log --pretty=format:"%H,%an,%ai,%s" >> commit-history.csv

      - name: Upload commit history as artifact
        uses: actions/upload-artifact@v4
        with:
          name: commit-history
          path: commit-history.csv

      - name: Extract diff of the commit
        run: |
          commit_id=$COMMIT_ID
          git diff $commit_id^ > diff_${commit_id}.txt

      - name: Save diff as artifact
        uses: actions/upload-artifact@v4
        with:
          name: commit-diff
          path: diff_${{ env.COMMIT_ID }}.txt

      - name: Upload Commit History to S3
        run: |
          aws s3 cp commit-history.csv $BASE_S3_PATH/codeChanges/commit-history.csv

      - name: Upload Repository Structure and Commit diff to S3 in YYYY/MM/DD Subfolder
        run: |
            aws s3 cp repo_structure_${{ env.COMMIT_ID }}.txt $BASE_S3_PATH/codeChanges/$COMMIT_DATE/repo_structure_${{ env.COMMIT_ID }}.txt
            aws s3 cp diff_${{ env.COMMIT_ID }}.txt $BASE_S3_PATH/codeChanges/$COMMIT_DATE/diff_${{ env.COMMIT_ID }}.txt
