name: Export Changes

on:
  workflow_call:
    inputs:
      bucket_name:
        required: true
        type: string
      aws_region:
        required: true
        type: string
      include_code:
        required: false
        type: string
        default: ''
      include_documentation:
        required: false
        type: string
        default: ''
    secrets:
      AWS_ACCESS_KEY_ID:
        required: true
      AWS_SECRET_ACCESS_KEY:
        required: true

jobs:
  export-structure:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set Commit SHA and Date as GitHub Variables
        run: |
          COMMIT_ID="${GITHUB_SHA}"
          
          # Ensure we have a commit SHA
          if [[ -z "$COMMIT_ID" ]]; then
            echo "No commit SHA found. Using latest commit from main branch."
            COMMIT_ID=$(git rev-parse HEAD)
          fi
          
          # Get commit date in YYYY/MM/DD format
          COMMIT_DATE=$(git show -s --format=%cd --date=format:'%Y/%m/%d' "$COMMIT_ID")
          echo "COMMIT_ID=$COMMIT_ID" >> $GITHUB_ENV
          echo "COMMIT_DATE=$COMMIT_DATE" >> $GITHUB_ENV

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ inputs.aws_region }}

#      - name: Get inputs for S3 upload
#        run: |
#          IFS=',' read -r -a INCLUDE_CODE <<< "${{ inputs.include_code }}"
#          IFS=',' read -r -a INCLUDE_DOCUMENTATION <<< "${{ inputs.include_documentations }}"
#
#      - name: Upload code to S3
#        run: |
#
#          aws s3 rm s3://${{ inputs.bucket_name }}/code/ --recursive
#          for path in "${INCLUDE_CODE[@]}"; do
#            aws s3 cp "$path" s3://${{ inputs.bucket_name }}/code/ --recursive
#          done
#
#      - name: Upload documentation to S3
#        run: |
#          if [ -n "${{ inputs.include_documentation_folders }}" ]; then
#            for path in "${INCLUDE_DOCUMENTATION[@]}"; do
#              aws s3 cp "$path" s3://${{ inputs.bucket_name }}/documentation/ --recursive
#            done
#          else
#            echo "No documentation folders specified for upload."
#          fi

      - name: Upload selected code and documentation to S3
        run: |
          set -e

          CODE_PATHS=$(echo "${{ inputs.include_code }}" | tr ',' ' ')
          DOC_PATHS=$(echo "${{ inputs.include_documentation }}" | tr ',' ' ')
          BUCKET="${{ inputs.bucket_name }}"

          # Funzione per upload: gestisce file e directory
          upload_to_s3() {
            local src=$1
            local dest_prefix=$2

            if [ -d "$src" ]; then
              # Directory → upload ricorsivo all'interno di dest_prefix/src/
              aws s3 cp "$src" "s3://$BUCKET/$dest_prefix/$src" --recursive
            elif [ -f "$src" ]; then
              # File → upload singolo
              aws s3 cp "$src" "s3://$BUCKET/$dest_prefix/$src"
            else
              echo "Warning: '$src' non esiste, skip."
            fi
          }

          # Pulizia cartelle di destinazione (per evitare residui)
          aws s3 rm "s3://$BUCKET/code/" --recursive || true
          aws s3 rm "s3://$BUCKET/docs/" --recursive || true

          # Upload dei percorsi di codice
          echo "Uploading code paths: $CODE_PATHS"
          for path in $CODE_PATHS; do
            upload_to_s3 "$path" "code"
          done

          # Upload dei percorsi di documentazione
          echo "Uploading doc paths: $DOC_PATHS"
          for path in $DOC_PATHS; do
            upload_to_s3 "$path" "docs"
          done

      - name: Generate repository structure
        run: |
          git ls-tree -r --name-only HEAD > repo_structure_${{ env.COMMIT_ID }}.txt  

      - name: Upload repository structure as artifact
        uses: actions/upload-artifact@v4
        with:
          name: repository-structure
          path: repo_structure_${{ env.COMMIT_ID }}.txt

      - name: Generate commit history
        run: |
          echo "Hash,Author,Date,Message" > commit-history.csv
          git log --pretty=format:"%H,%an,%ai,%s" >> commit-history.csv

      - name: Upload commit history as artifact
        uses: actions/upload-artifact@v4
        with:
          name: commit-history
          path: commit-history.csv

      - name: Extract diff of the commit
        run: |
          commit_id=$COMMIT_ID
          git diff $commit_id^ > diff_${commit_id}.txt

      - name: Save diff as artifact
        uses: actions/upload-artifact@v4
        with:
          name: commit-diff
          path: diff_${{ env.COMMIT_ID }}.txt

      - name: Upload Commit History to S3
        run: |
          aws s3 cp commit-history.csv s3://${{ inputs.bucket_name }}/codeChanges/commit-history.csv

      - name: Upload Repository Structure and Commit diff to S3 in YYYY/MM/DD Subfolder
        run: |
            aws s3 cp repo_structure_${{ env.COMMIT_ID }}.txt s3://${{ inputs.bucket_name }}/codeChanges/$COMMIT_DATE/repo_structure_${{ env.COMMIT_ID }}.txt
            aws s3 cp diff_${{ env.COMMIT_ID }}.txt s3://${{ inputs.bucket_name }}/codeChanges/$COMMIT_DATE/diff_${{ env.COMMIT_ID }}.txt
